



spring.application.name=MTCS-REALTIME-SERVER
server.port=20116

eureka.instance.preferIpAddress=true
eureka.client.serviceUrl.defaultZone=${REGISTRY_SERVICE_URL}

spring.cloud.config.discovery.enabled=true
spring.cloud.config.discovery.serviceId=config
spring.cloud.config.username=admin
spring.cloud.config.password=Register&1003
spring.cloud.config.fail-fast=true
spring.cloud.config.name=commonConfig,bigdataDBConfig,mybatisPlusConfig,\
  mtcs-realtime-server,mtcs-realtime-server-admin,sys-permission
spring.cloud.config.label=master
spring.cloud.config.profile=pro

feign.client.config.default.connect-timeout=20000
feign.client.config.default.read-timeout=20000


log.dir.path=${LOG_DIR_PATH}


logging.level.com.netflix.discovery.shared.resolver.aws.ConfigClusterResolver=WARN


############################################ xxl-job 配置
### xxl-job admin address list, such as "http://address" or "http://address01,http://address02"
xxl.job.admin.addresses=${xxl.job.admin.addresses}
#xxl.job.admin.addresses=http://192.168.254.40:20121
#xxl.job.admin.addresses=http://192.168.65.1:20121
### xxl-job executor address
xxl.job.executor.appname=mtcs-realtime-server
xxl.job.executor.ip=
xxl.job.executor.port=${xxl.job.executor.port}
#xxl.job.executor.port=20119
### xxl-job, access token
xxl.job.accessToken=${xxl.job.accessToken}
#xxl.job.accessToken=cn-ffcs-mtcs
### xxl-job log path
xxl.job.executor.logpath=${log.dir.path}/xxl-job/${spring.application.name}
### xxl-job log retention days
xxl.job.executor.logretentiondays=30

############################################ 配置kafka
##broker配置
spring.kafka.bootstrap-servers=${spring.kafka.bootstrap-servers}
#spring.kafka.bootstrap-servers=bigdata-hadoop-data--1.novalocal:6667,bigdata-hadoop-data--2.novalocal:6667,bigdata-hadoop-data--3.novalocal:6667
#测试环境
#spring.kafka.bootstrap-servers=134.129.98.61:6667,134.129.98.62:6667,134.129.98.63:6667,134.129.98.64:6667
#正式环境
#spring.kafka.bootstrap-servers=134.130.69.55:6667,134.130.69.56:6667,134.130.69.57:6667,134.130.69.58:6667
#spring.kafka.jaas.enabled=false
#spring.kafka.jaas.login-module=com.sun.security.auth.module.Krb5LoginModule
#spring.kafka.jaas.control-flag=required
#spring.kafka.properties.security.protocol = SASL_PLAINTEXT
#spring.kafka.properties.sasl.mechanism = GSSAPI
#spring.kafka.properties.sasl.kerberos.service.name = kafka


## consumer配置
spring.kafka.consumer.bootstrap-servers=${spring.kafka.bootstrap-servers}
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
# 指定一个默认的组名
spring.kafka.consumer.group-id=${spring.kafka.consumer.group-id}
# earliest:当各分区下有已提交的offset时，
#   从提交的offset开始消费；无提交的offset时，从头开始消费
# latest:当各分区下有已提交的offset时，
#   从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
# none:topic各分区都存在已提交的offset时，
#   从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
spring.kafka.consumer.auto-offset-reset=latest
## producer配置
spring.kafka.producer.bootstrap-servers=${spring.kafka.bootstrap-servers}
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
# 批量抓取
spring.kafka.producer.batch-size=65536
# 缓存容量
spring.kafka.producer.buffer-memory=524288


send.ssh.server.topic=${send.ssh.server.topic}

receive.ssh.server.topic=${receive.ssh.server.topic}











